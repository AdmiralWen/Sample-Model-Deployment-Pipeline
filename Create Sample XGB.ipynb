{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sample XGB Model with Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "import pickle as pk\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data:\n",
    "data0 = pd.read_excel('sample_training_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data0.copy()\n",
    "\n",
    "# Create date-length variable:\n",
    "data['date1'] = pd.to_datetime(data['date1'])\n",
    "data['date2'] = pd.to_datetime(data['date2'])\n",
    "data['date_diff'] = (data['date2'] - data['date1']).map(lambda x: x.days)\n",
    "\n",
    "# Create new cat3_1D variable (simulates SIC-1D):\n",
    "data['cat3_1D'] = data['cat3'].map(lambda x: str(x)[0], na_action = 'ignore')\n",
    "\n",
    "# Handle dummy_rating:\n",
    "data['dummy_rating_cat'] = np.where(data['dummy_rating'].str.contains('A'), 1, -1)\n",
    "data['dummy_rating_cat'] = np.where(pd.isnull(data['dummy_rating']), np.nan, data['dummy_rating_cat'])\n",
    "\n",
    "# Handle num1 and num5:\n",
    "data['num1'] = data['num1'] - 1900\n",
    "data['num5'] = data['num5'].map(lambda x: log(x + 1) if x > 0 else 0, na_action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE cat1, cat2, and cat3_1D:\n",
    "for f in ['cat1', 'cat2', 'cat3_1D']:\n",
    "    # Create OHE feature:\n",
    "    feature = data[f].map(str, na_action = 'ignore').fillna('null')\n",
    "    onehot_encoder = OneHotEncoder(sparse = False, categories = 'auto')\n",
    "    onehot_encoder.fit(feature.values.reshape(len(data), 1))\n",
    "    f_enc = onehot_encoder.transform(feature.values.reshape(len(data), 1))\n",
    "\n",
    "    # Save OHE object:\n",
    "    with open('Sample XGB Model/ohe_{}.pkl'.format(f), 'wb') as pickle_file:\n",
    "        pk.dump(onehot_encoder, pickle_file)\n",
    "\n",
    "    # Append new OHE features and drop old:\n",
    "    f_col_names = onehot_encoder.get_feature_names([f])\n",
    "    data = pd.concat([data.drop(f, axis = 1).reset_index(drop = True), pd.DataFrame(f_enc, columns = f_col_names)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA:\n",
    "pca_features = 4\n",
    "pca_fields = ['pca1', 'pca2', 'pca3', 'pca4', 'pca5', 'pca6', 'pca7', 'pca8', 'pca9', 'pca10']\n",
    "ph_cols = data[pca_fields]\n",
    "\n",
    "# Missing values imputation:\n",
    "missing_imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\n",
    "missing_imputer.fit(ph_cols)\n",
    "ph_cols = missing_imputer.transform(ph_cols)\n",
    "with open('Sample XGB Model/missing_imputer.pkl', 'wb') as pickle_file:\n",
    "    pk.dump(missing_imputer, pickle_file)\n",
    "    \n",
    "# Variable scaling:\n",
    "variable_scaler = StandardScaler()\n",
    "variable_scaler.fit(ph_cols)\n",
    "ph_cols_std = variable_scaler.transform(ph_cols)\n",
    "with open('Sample XGB Model/variable_scaler.pkl', 'wb') as pickle_file:\n",
    "    pk.dump(variable_scaler, pickle_file)\n",
    "\n",
    "# Create PCA features:\n",
    "pca = PCA(n_components = pca_features)\n",
    "pca.fit(ph_cols_std)\n",
    "pca_cols = pca.transform(ph_cols_std)\n",
    "with open('Sample XGB Model/pca.pkl', 'wb') as pickle_file:\n",
    "    pk.dump(pca, pickle_file)\n",
    "\n",
    "# Combine with overall dataset:\n",
    "pca_cols_df = pd.DataFrame(data = pca_cols, columns = ['pca_out{}'.format(i) for i in range(pca_features)])\n",
    "data = data.drop(pca_fields, axis = 1).reset_index(drop = True)\n",
    "data = pd.concat([data, pca_cols_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2356, 40)\n"
     ]
    }
   ],
   "source": [
    "# Drop irrelevant fields:\n",
    "data.drop(['row_id', 'dummy_name', 'date1', 'date2', 'cat3', 'dummy_rating'], axis = 1, inplace = True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train sample XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable and D-Matrix:\n",
    "train_y = data['target_var'].fillna(0)\n",
    "train_x = data.drop('target_var', axis = 1)\n",
    "train_dm = xgb.DMatrix(train_x.values, train_y.values, feature_names = train_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model:\n",
    "xgb_params = {\n",
    "    'objective': 'count:poisson',\n",
    "    'eval_metric': 'poisson-nloglik',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'gamma': 0.1,\n",
    "    'alpha': 0.1,\n",
    "    'nthread': 8\n",
    "}\n",
    "\n",
    "xgb_mod = xgb.train(dtrain = train_dm, params = xgb_params, num_boost_round = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save:\n",
    "xgb_mod.save_model('Sample XGB Model/sample_xgb.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7154076], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_order = ['num1', 'num2', 'num3', 'num4', 'num5', 'binary1', 'binary2', 'binary3', 'date_diff', 'dummy_rating_cat', 'cat1_I', 'cat1_N', 'cat1_P', 'cat1_R', 'cat1_null',\n",
    "             'cat2_AMP', 'cat2_ECR', 'cat2_EP', 'cat2_FL', 'cat2_FLD', 'cat2_MFP', 'cat2_MN', 'cat2_N1C', 'cat2_PE', 'cat2_PP', 'cat3_1D_1', 'cat3_1D_2', 'cat3_1D_3',\n",
    "             'cat3_1D_4', 'cat3_1D_5', 'cat3_1D_6', 'cat3_1D_7', 'cat3_1D_8', 'cat3_1D_9', 'cat3_1D_null', 'pca_out0', 'pca_out1', 'pca_out2', 'pca_out3']\n",
    "\n",
    "rec = train_x.iloc[1:2]\n",
    "rec = rec[col_order]\n",
    "\n",
    "rec_dm = xgb.DMatrix(rec.values, feature_names = train_x.columns)\n",
    "xgb_mod.predict(rec_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'binary1': 1,\n",
       "  'binary2': 1,\n",
       "  'binary3': 1,\n",
       "  'cat1': 'R',\n",
       "  'cat2': 'PP',\n",
       "  'cat3': 3553.0,\n",
       "  'date1': Timestamp('2018-02-05 00:00:00'),\n",
       "  'date2': Timestamp('2019-02-05 00:00:00'),\n",
       "  'dummy_name': '2Zl*3Nb&',\n",
       "  'dummy_rating': 'A1',\n",
       "  'num1': nan,\n",
       "  'num2': 51.0,\n",
       "  'num3': nan,\n",
       "  'num4': 4.0,\n",
       "  'num5': nan,\n",
       "  'pca1': 1.92,\n",
       "  'pca10': 1.05,\n",
       "  'pca2': 1.28,\n",
       "  'pca3': 1.1,\n",
       "  'pca4': 1.4,\n",
       "  'pca5': 1.03,\n",
       "  'pca6': 1.73,\n",
       "  'pca7': 1.65,\n",
       "  'pca8': 1.98,\n",
       "  'pca9': 1.86,\n",
       "  'row_id': 2,\n",
       "  'target_var': 0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data0.iloc[1:2].to_dict('records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
